{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_MAB_movielens_thompson.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/aLwW3XFy2Pw2Dx4otnjV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstanisl/mlprague-2021/blob/main/04_MAB_movielens_thompson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv1UgQL6-W4y"
      },
      "source": [
        "# Multi-Arm Bandits - Thompson Sampling\n",
        "\n",
        "Thompson Sampling is a straightforward yet effective method to addressing the exploration-exploitation dilemma in reinforcement/online learning. Up until now, all of the techniques weâ€™ve seen for tackling the Bandit Problem have selected actions based on the current averages of the rewards received. Thompson Sampling (also sometimes referred to as the Bayesian Bandits algorithm) takes a slightly different approach; rather than just refining an estimate of the mean reward, it extends this to instead build up a probability model from the obtained rewards then samples from this to choose an action.\n",
        "\n",
        "#### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWLZjJq1Iljq"
      },
      "source": [
        "!rm -f ./utils.py\n",
        "!wget --no-check-certificate --no-cache --no-cookies \\\n",
        "    https://raw.githubusercontent.com/pstanisl/mlprague-2021/main/utils.py \\\n",
        "    -O ./utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HrMni5TIxls"
      },
      "source": [
        "##### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI1Fg9CJ-TuE"
      },
      "source": [
        "from scipy.stats import norm\n",
        "from tqdm.notebook import trange\n",
        "from typing import Any, Dict, List, Text, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "\n",
        "from utils import load_movielens_data\n",
        "from utils import plot_actions, plot_cumsum, plot_pdf, plot_regret\n",
        "\n",
        "# Apply the default theme\n",
        "sns.set_theme()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npt58UZI-e0z"
      },
      "source": [
        "#### Downloading the [MovieLens](https://grouplens.org/datasets/movielens/) (100K) dataset.\n",
        "\n",
        "**Dataset info**\n",
        "\n",
        "MovieLens data sets were collected by the GroupLens Research Project\n",
        "at the University of Minnesota.\n",
        "\n",
        "This data set consists of:\n",
        "* 100,000 ratings (1-5) from 943 users on 1682 movies.\n",
        "* Each user has rated at least 20 movies.\n",
        "* Simple demographic info for the users (age, gender, occupation, zip)\n",
        "\n",
        "The data was collected through the MovieLens web site\n",
        "(movielens.umn.edu) during the seven-month period from September 19th,\n",
        "1997 through April 22nd, 1998. This data has been cleaned up - users\n",
        "who had less than 20 ratings or did not have complete demographic\n",
        "information were removed from this data set. Detailed descriptions of\n",
        "the data file can be found at the end of this file.\n",
        "\n",
        "Neither the University of Minnesota nor any of the researchers\n",
        "involved can guarantee the correctness of the data, its suitability\n",
        "for any particular purpose, or the validity of results based on the\n",
        "use of the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yC7iZ00-cXB"
      },
      "source": [
        "print(\"Downloading movielens data...\")\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    http://files.grouplens.org/datasets/movielens/ml-100k.zip \\\n",
        "    -O ./movielens.zip\n",
        "\n",
        "zip_ref = zipfile.ZipFile('movielens.zip', \"r\")\n",
        "zip_ref.extractall()\n",
        "\n",
        "print(\"Done. Dataset contains:\")\n",
        "print(zip_ref.read('ml-100k/u.info').decode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrBEsPaPOlKc"
      },
      "source": [
        "#### Parameters -- Feel Free to Play Around"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUX1tZbOmiU"
      },
      "source": [
        "RANK_K = 20 # @param {type:\"integer\"}\n",
        "NUM_ACTIONS = 20 # @param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gctzoyb_XAc"
      },
      "source": [
        "## Environment\n",
        "\n",
        "Implementation of the environment uses **MovieLens 100K dataset**. As described above, the dataset contains 100000 ratings from 943 users and 1682 movies. The environment can consider only the first $n$ of the dataset's movies. It can be set-up by `num_actions`. The number of \"known\" movies for the environment is equal to actions/arms.\n",
        "\n",
        "> Users without a rating (after selecting first $n$ movies) are removed from the environment.\n",
        "\n",
        "> In every step, the batch of users will be selected randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-1U_tIg_U9K"
      },
      "source": [
        "class MovielensEnvironment(object):\n",
        "  def __init__(\n",
        "      self, \n",
        "      data_dir: Text,\n",
        "      rank_k: int, \n",
        "      batch_size: int = 1,\n",
        "      num_movies: int = 20\n",
        "  ):\n",
        "    \"\"\"Initializes the MovieLens Bandit environment.\n",
        "    Args:\n",
        "      data_dir: (string) Directory where the data lies (in text form).\n",
        "      rank_k : (int) Which rank to use in the matrix factorization.\n",
        "      batch_size: (int) Number of observations generated per call.\n",
        "      num_movies: (int) Only the first `num_movies` movies will be used by the\n",
        "        environment. The rest is cut out from the data.\n",
        "    \"\"\"\n",
        "    self._num_actions = num_movies\n",
        "    self._batch_size = batch_size\n",
        "    self._context_dim = rank_k\n",
        "\n",
        "    # Compute the matrix factorization.\n",
        "    self._data_matrix = load_movielens_data(data_dir)\n",
        "    # Keep only the first items.\n",
        "    self._data_matrix = self._data_matrix[:, :num_movies]\n",
        "    # Filter the users with no iterm rated.\n",
        "    nonzero_users = list(np.nonzero(np.sum(self._data_matrix, axis=1) > 0.0)[0])\n",
        "    self._data_matrix = self._data_matrix[nonzero_users, :]\n",
        "    self._effective_num_users = len(nonzero_users)\n",
        "\n",
        "    # Compute the SVD.\n",
        "    u, s, vh = np.linalg.svd(self._data_matrix, full_matrices=False)\n",
        "    # Keep only the largest singular values.\n",
        "    self._u_hat = u[:, :rank_k] * np.sqrt(s[:rank_k])\n",
        "    self._v_hat = np.transpose(\n",
        "        np.transpose(vh[:rank_k, :]) * np.sqrt(s[:rank_k]))\n",
        "    self._approx_ratings_matrix = np.matmul(self._u_hat, self._v_hat)\n",
        "\n",
        "    self._current_users = np.zeros(batch_size)\n",
        "    self._previous_users = np.zeros(batch_size)\n",
        "\n",
        "    self._optimal_action_table = np.argmax(\n",
        "        self._approx_ratings_matrix, axis=1)\n",
        "    self._optimal_reward_table = np.max(\n",
        "        self._approx_ratings_matrix, axis=1)\n",
        "    \n",
        "    #self._params = params\n",
        "    #self._observe()\n",
        "    self.reset()\n",
        "\n",
        "  @property\n",
        "  def batch_size(self):\n",
        "    return self._batch_size\n",
        "\n",
        "  @property\n",
        "  def best_action(self) -> int:\n",
        "    return np.argmax(np.sum(env._data_matrix, axis=0))\n",
        "\n",
        "  @property\n",
        "  def n_actions(self) -> int:\n",
        "    return self._data_matrix.shape[1]\n",
        "\n",
        "  def reset(self):\n",
        "    return self._observe()\n",
        "\n",
        "  def _observe(self) -> np.ndarray:\n",
        "    \"\"\"Returns the u vectors of a random sample of users.\"\"\"\n",
        "    sampled_users = random.sample(\n",
        "        range(self._effective_num_users), self._batch_size)\n",
        "    self._previous_users = self._current_users\n",
        "    self._current_users = sampled_users\n",
        "    batched_observations = self._u_hat[sampled_users]\n",
        "    return batched_observations\n",
        "\n",
        "  def step(self, action: List[int]) -> Tuple[int, float]:\n",
        "    \"\"\"Computes the reward for the input actions.\"\"\"\n",
        "    rewards = []\n",
        "    for i, j in zip(self._current_users, action):\n",
        "      rewards.append(self._approx_ratings_matrix[i, j])\n",
        "    self._observe()\n",
        "    return np.array(rewards)\n",
        "\n",
        "  def compute_optimal_action(self):\n",
        "    return self._optimal_action_table[self._previous_users]\n",
        "\n",
        "  def compute_optimal_reward(self):\n",
        "    return self._optimal_reward_table[self._previous_users]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3VUoO2tJcau"
      },
      "source": [
        "Now we are equipped to initialize our environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_PBr_ks_bP4"
      },
      "source": [
        "env = MovielensEnvironment(\n",
        "    './ml-100k/u.data', rank_k=RANK_K, batch_size=1, num_movies=NUM_ACTIONS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H1uCkGXJgrK"
      },
      "source": [
        "Below we can check what this environment produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMoj8Sz9Jhi5"
      },
      "source": [
        "env.reset()\n",
        "\n",
        "action = np.zeros(1, dtype=np.int32)\n",
        "reward = env.step(action)\n",
        "\n",
        "print(f'For users={env._previous_users}, we selected action={action} (optimal={env.compute_optimal_action()})')\n",
        "print(f'For users={env._previous_users}, we received reward={reward} (optimal={env.compute_optimal_reward()})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESaZ6StB_hEN"
      },
      "source": [
        "## Policy\n",
        "\n",
        "\n",
        "The Thompson sampling algorithm simply samples actions according to the posterior probability they are optimal. In particular, actions are chosen randomly at time $t$ according to the sampling distribution $\\pi_{t}^{TS} = \\mathbb{P}(A^{\\ast} = Â·| \\mathcal{F}_{t} )$. By definition, this means that for each $a \\in \\mathcal{A}$, $\\mathbb{P}(A_{t} = a|\\mathcal{F}_{t}) = \\mathbb{P}(A^{\\ast} = a|\\mathcal{F}_{t})$. This algorithm is sometimes called probability matching because the action selection distribution is matched to the posterior distribution of the optimal action.\n",
        "\n",
        "In our example, we use Normal distribussion for actions, i.e.,\n",
        "\n",
        "$$\n",
        "\\hat{a}_{t}^{\\ast} = \\mathcal{N}\\left(\\mu, \\sigma^{2} \\right) = \\mathcal{N}\\left(\\mu_{0}, \\frac{1}{\\tau_{0}} \\right)\n",
        "$$\n",
        "\n",
        "<br/>\n",
        "\n",
        "**TASK**: Add code to sample the next batch of the actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md2gEPhT_eOp"
      },
      "source": [
        "class GaussianPolicy(object):\n",
        "  def __init__(self, tau: List[float], mu: List[float], batch_size: int = 1):\n",
        "    self._tau0 = tau\n",
        "    self._mu0 = mu\n",
        "    self._n = len(tau)\n",
        "    self._batch_size = batch_size\n",
        "\n",
        "  def action(self) -> int:\n",
        "    sample = # YOUR CODE HERE\n",
        "    return # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHAtGHCqYoRD"
      },
      "source": [
        "Below we can check what this policy produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2qJyjea__Xi"
      },
      "source": [
        "p = GaussianPolicy([1, 10, 5], [1, 1, 1], batch_size=10)\n",
        "print(f'GaussianPolicy - action: {p.action()}')\n",
        "\n",
        "# Example of the output\n",
        "# GaussianPolicy - action: [2 2 1 2 0 0 1 0 2 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqV8AAqkErwt"
      },
      "source": [
        "## Agent\n",
        "\n",
        "The primary function of the agent is to update policy parameters. In case of Thompson sampling, it is mainly update $\\mu_{0}$ and $\\tau_{0}$\n",
        "\n",
        "$$\n",
        "\\tau_{0} \\leftarrow \\tau_{0} + n\\tau, \\\\\n",
        "\\mu_{0} \\leftarrow \\frac{\\tau_{0}\\mu_{0} + \\tau Q}{\\tau_{0} + n\\tau},\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "* $\\tau$ is the precision of the action, which in our case is just $1$,\n",
        "* $n$ is the number of times that action $a$ has been selected,\n",
        "* $Q$ is estimated value of action $a$ at time step $t$,\n",
        "* $\\mu_{0}$ is the estimated mean (the mean of the distribution used to model the output),\n",
        "* $\\tau_{0}$ is the precision of the distribution used to model the output.\n",
        "\n",
        "<br/>\n",
        "\n",
        "**TASK**: Add code for updating distribution parameters $\\mu_{0}$ and $\\tau_{0}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmjZKU68ATjf"
      },
      "source": [
        "class GaussianThompsonSamplingAgent(object):\n",
        "  \n",
        "  def __init__(self, n: int, batch_size: int = 1):\n",
        "    self._batch_size = batch_size\n",
        "    self._n = n\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "    self.policy = GaussianPolicy(self._tau0, self._mu0, batch_size=batch_size)\n",
        "\n",
        "  def reset(self):\n",
        "    self._tau0 = # YOUR CODE HERE\n",
        "    self._mu0 = # YOUR CODE HERE\n",
        "    self._Q = # YOUR CODE HERE\n",
        "\n",
        "    self._counts = # YOUR CODE HERE\n",
        "    self._values = # YOUR CODE HERE\n",
        "\n",
        "  def train(self, experience: Dict[str, float]):\n",
        "    \"\"\"Update policy parameters.\n",
        "\n",
        "    Args:\n",
        "      experience: dictionary with a single action and reward\n",
        "    \"\"\"\n",
        "    action = experience['action']\n",
        "    reward = experience['reward']\n",
        "    # Update average/mean value/reward for chosen action\n",
        "    \n",
        "    # YOUR CODE GOES HERE\n",
        "    \n",
        "    # END OF YOUR CODE\n",
        "        \n",
        "    self._mu0[action] = # YOUR CODE HERE       \n",
        "    self._tau0[action] = # YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJYrbePibc4z"
      },
      "source": [
        "Below we can check how the training affects rewards estimation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH3uy7NwF20H"
      },
      "source": [
        "a = GaussianThompsonSamplingAgent(3, batch_size=1)\n",
        "\n",
        "experience = {'action': 2, 'reward': 1}\n",
        "a.train(experience)\n",
        "print(f'Q={a._values}')\n",
        "print(f'tau={a._tau0}')\n",
        "\n",
        "experience = {'action': 2, 'reward': 0}\n",
        "a.train(experience)\n",
        "print(f'Q={a._values}')\n",
        "print(f'tau={a._tau0}')\n",
        "\n",
        "# Expected output\n",
        "#Q=[0.0, 0.0, 1.0]\n",
        "#tau=[0.0001, 0.0001, 1.0001]\n",
        "#Q=[0.0, 0.0, 0.5]\n",
        "#tau=[0.0001, 0.0001, 2.0000999999999998]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5zv5gQ4GlTM"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now we put together all the components that we introduced above: the environment, the policy, and the agent. We run the policy on the environment and output training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w28hXUbGEnG"
      },
      "source": [
        "def run(environment: object, agent: object, trials=100):\n",
        "  trajectory = []\n",
        "\n",
        "  experience = {'trial': 0, 'action': -1, 'observation': 0, 'reward': 0}\n",
        "\n",
        "  for i in range(trials):\n",
        "    experience['trial'] = i + 1\n",
        "    actions = agent.policy.action()\n",
        "    rewards = environment.step(actions)\n",
        "    optimal_rewards = environment.compute_optimal_reward()\n",
        "\n",
        "    for action, reward, optimal_reward in zip(actions, rewards, optimal_rewards):\n",
        "      experience['action'] = action\n",
        "      experience['reward'] = reward\n",
        "      experience['regret'] = optimal_reward - reward\n",
        "\n",
        "      agent.train(experience)\n",
        "\n",
        "      trajectory.append(experience.copy())\n",
        "    \n",
        "  df_trajectory = pd.DataFrame.from_dict(trajectory)\n",
        "  df_cumsum = df_trajectory.groupby('action')['reward'].cumsum()\n",
        "  df_trajectory = df_trajectory.assign(cum_sum=df_trajectory['reward'].cumsum())\n",
        "  df_trajectory = df_trajectory.assign(action_cum_sum=df_cumsum)\n",
        "\n",
        "  return df_trajectory.astype({'action': 'int32', 'trial': 'int32'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaEhg7ZtYShe"
      },
      "source": [
        "Down below is the code for creating all necessary instances. We have here a few parameters we can play with. `num_iterations` specifies how many times we run the trainer loop, `batch_size` defines how many actions are generated through one step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pheA13BFGrwa"
      },
      "source": [
        "batch_size =   32 # @param {type:\"integer\"}\n",
        "num_iterations =   150 # @param {type:\"integer\"}\n",
        "\n",
        "environment = MovielensEnvironment(\n",
        "  './ml-100k/u.data', \n",
        "  rank_k=RANK_K, \n",
        "  batch_size=batch_size, \n",
        "  num_movies=NUM_ACTIONS\n",
        ")\n",
        "step = environment.reset()\n",
        "\n",
        "agent = GaussianThompsonSamplingAgent(\n",
        "    environment.n_actions, \n",
        "    batch_size=environment.batch_size)\n",
        "experience = {'action': [-1], 'reward': [0]}\n",
        "\n",
        "df_trajectory = run(environment, agent, trials=num_iterations)\n",
        "\n",
        "print(f'\\Q={agent._values}')\n",
        "print(f'N={agent._counts}')\n",
        "print(f'best action={np.argmax(agent._values)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4nlXNGOP4OO"
      },
      "source": [
        "Now let's see the result. After running the last code snippet, the resulting plot (hopefully) shows that the average regret is going down as the agent is trained and the policy gets better in figuring out what the right action is, given the observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaiXdBugP5bo"
      },
      "source": [
        "params = {\n",
        "    'mu': np.array(agent._mu0), \n",
        "    'tau': np.array(agent._tau0),\n",
        "    'dist': 'Normal',\n",
        "    'algorithm': 'Thompson Sampling'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQp1dSQP4tDS"
      },
      "source": [
        "plot_regret(df_trajectory.groupby('trial').mean()['regret'], params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-tcq437G1dq"
      },
      "source": [
        "plot_pdf(params, type=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FewScO7ZK35s"
      },
      "source": [
        "Let's see the selection's rate of the actions during the experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7JaMOcoP1nn"
      },
      "source": [
        "plot_cumsum(df_trajectory, params, show_actions=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQsfYBZaIRoy"
      },
      "source": [
        "#### Multiple runs\n",
        "\n",
        "Because our environment selects users randomly, each run can produce different results. Let's what are the average results through multiple runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zymjka5pHZmL"
      },
      "source": [
        "def experiment(epochs: int = 1, trials: int = 10, batch_size: int = 1):\n",
        "  trajectories = []\n",
        "  \n",
        "  environment = MovielensEnvironment(\n",
        "      './ml-100k/u.data', \n",
        "      rank_k=RANK_K, \n",
        "      batch_size=batch_size, \n",
        "      num_movies=NUM_ACTIONS)\n",
        "  params = {\n",
        "      'best_action': environment.best_action,\n",
        "      'mu': [],\n",
        "      'tau': [],\n",
        "  }\n",
        "  \n",
        "  for epoch in trange(epochs):\n",
        "    step = environment.reset()\n",
        "    agent = GaussianThompsonSamplingAgent(\n",
        "        environment.n_actions, batch_size=environment.batch_size)\n",
        "    \n",
        "    df = run(environment, agent, trials=trials)\n",
        "    df['epoch'] = epoch + 1\n",
        "\n",
        "    params['mu'].append(agent._mu0)\n",
        "    params['tau'].append(agent._tau0)\n",
        "\n",
        "    trajectories.append(df)\n",
        "\n",
        "  df_trajectory = pd.concat(trajectories, ignore_index=True)\n",
        "\n",
        "  return df_trajectory.astype({'action': 'int32', 'trial': 'int32'}), params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv1gcuWqYL6W"
      },
      "source": [
        "Compared to a single run, we have one extra parameter. `epochs` controls the number of independent runs of the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8zMRKtBI5w1"
      },
      "source": [
        "batch_size =   32# @param {type:\"integer\"}\n",
        "epochs =  25# @param {type: \"integer\"}\n",
        "num_iterations =   100# @param {type:\"integer\"}\n",
        "\n",
        "df_trajectory, params = experiment(\n",
        "    epochs=epochs, \n",
        "    trials=num_iterations, \n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26-U-PTwNNq6"
      },
      "source": [
        "Now let's see the average results after running multiple runs. The resulting regret plot (hopefully) shows that the average regret is going down as the agent is trained and the policy gets better in figuring out what the right action is, given the observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--w6BtLPNRVn"
      },
      "source": [
        "params = {\n",
        "    'mu': np.array(params['mu']).mean(axis=0), \n",
        "    'tau': np.array(params['tau']).mean(axis=0),\n",
        "    'dist': 'Normal',\n",
        "    'algorithm': 'Thompson Sampling'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyQPAbUS4qeH"
      },
      "source": [
        "plot_regret(df_trajectory.groupby('trial').mean()['regret'], params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_SIARxCJBIL"
      },
      "source": [
        "plot_pdf(params, type=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BznCY7qqJaGl"
      },
      "source": [
        "plot_cumsum(df_trajectory, params, show_actions=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CweRU1POK2Fp"
      },
      "source": [
        "Let's see the selection's rate of the actions during the experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGkfSbi-QQGo"
      },
      "source": [
        "plot_actions(df_trajectory, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odRoTol1Qalg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}