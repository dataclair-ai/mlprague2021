{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_CMAB_movielens_linear_tf.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP8g2HEr4RjCJDZGykUxSfF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pstanisl/mlprague-2021/blob/main/05_CMAB_movielens_linear_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku9blLrybt_E"
      },
      "source": [
        "# Linear Contextual Multi-Armed Bandits \n",
        "\n",
        "From now we will use [TensorFlow Agents](https://www.tensorflow.org/agents), so so it's probably appropriate to say something about this library. Agents is a library for reinforcement learning in TensorFlow.  \n",
        "\n",
        "> TF-Agents makes designing, implementing, and testing new RL algorithms easier by providing well-tested modular components that can be modified and extended. It enables fast code iteration with good test integration and benchmarking.\n",
        "\n",
        "It provides API for creating all aspects necessary for reinforcement learning with Tensorflow, example of API can be seen below.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "\n",
        "q_net = q_network.QNetwork(\n",
        "  train_env.observation_spec(),\n",
        "  train_env.action_spec(),\n",
        "  fc_layer_params=(100,))\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "  train_env.time_step_spec(),\n",
        "  train_env.action_spec(),\n",
        "  q_network=q_net,\n",
        "  optimizer=optimizer,\n",
        "  td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "  train_step_counter=tf.Variable(0))\n",
        "\n",
        "agent.initialize()\n",
        "```\n",
        "\n",
        "As Multi-Armed Bandits can be seen as a special case of RL, TF-Agents contains also building blocks for MAB, especially for Contextual Multi-Armed Bandits (CMAB). ☺️\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6fNTjpnu6eS"
      },
      "source": [
        "#### Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbVPxLuUR25T"
      },
      "source": [
        "!pip install tf-agents -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2MMqktAoX5e"
      },
      "source": [
        "!rm -f ./utils.py\n",
        "!wget --no-check-certificate --no-cache --no-cookies \\\n",
        "    https://raw.githubusercontent.com/pstanisl/mlprague-2021/main/utils.py \\\n",
        "    -O ./utils.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02ZZepbLdTS7"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUEquHXHRwwB"
      },
      "source": [
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
        "import tensorflow_probability as tfp\n",
        "import zipfile\n",
        "\n",
        "from tqdm.notebook import trange\n",
        "from typing import Optional, Sequence, Text, Tuple\n",
        "\n",
        "from tf_agents.agents import data_converter\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.bandits.agents import linear_bandit_agent as lin_agent\n",
        "from tf_agents.bandits.agents import utils as bandit_utils\n",
        "from tf_agents.bandits.environments import environment_utilities\n",
        "from tf_agents.bandits.environments import bandit_py_environment\n",
        "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
        "from tf_agents.bandits.policies import linalg\n",
        "from tf_agents.bandits.policies import linear_bandit_policy\n",
        "from tf_agents.bandits.policies import policy_utilities\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import policy_step\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.typing import types\n",
        "\n",
        "from utils import load_movielens_data, plot_regret, trajectory_for_bandit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHbSBjJoDVGz"
      },
      "source": [
        "#### Banner example - a bit dirty ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pQURB_g5EZB"
      },
      "source": [
        "class BannerEnvironment(bandit_py_environment.BanditPyEnvironment):\r\n",
        "\r\n",
        "  def __init__(self, ctrs: Sequence[float]):\r\n",
        "    action_spec = array_spec.BoundedArraySpec(\r\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=len(ctrs) - 1, name='action')\r\n",
        "    observation_spec = array_spec.BoundedArraySpec(\r\n",
        "        shape=(1,), dtype=np.float32, minimum=0., maximum=1., name='observation')\r\n",
        "    \r\n",
        "    self.ctrs = ctrs\r\n",
        "\r\n",
        "    super(BannerEnvironment, self).__init__(observation_spec, action_spec)\r\n",
        "\r\n",
        "  def _observe(self):\r\n",
        "    self._observation = np.random.rand(1)\r\n",
        "    return self._observation\r\n",
        "\r\n",
        "  def _apply_action(self, action):\r\n",
        "    return self.ctrs[action] > self._observation[0]\r\n",
        "\r\n",
        "\r\n",
        "class GreedyPolicy(tf_policy.TFPolicy):\r\n",
        "  def __init__(self, n, values):\r\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\r\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=n - 1)\r\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\r\n",
        "        shape=(1,), dtype=tf.float64, minimum=0., maximum=1)\r\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\r\n",
        "    \r\n",
        "    self._values = values\r\n",
        "\r\n",
        "    super(GreedyPolicy, self).__init__(\r\n",
        "        time_step_spec=time_step_spec, action_spec=action_spec)\r\n",
        "\r\n",
        "  def _variables(self) -> Sequence[tf.Variable]:\r\n",
        "    return []\r\n",
        "\r\n",
        "  def _action(\r\n",
        "      self, \r\n",
        "      time_step: ts.TimeStep, \r\n",
        "      policy_state: types.NestedTensor = (), \r\n",
        "      seed: Optional[types.Seed] = None) -> policy_step.PolicyStep:\r\n",
        "    action = tf.cast(tf.reshape(tf.math.argmax(self._values), [1]), dtype=tf.int32)\r\n",
        "    return policy_step.PolicyStep(action, policy_state)\r\n",
        "\r\n",
        "\r\n",
        "class GreedyAgent(tf_agent.TFAgent):\r\n",
        "  def __init__(self, n: int):\r\n",
        "    self._n = n\r\n",
        "    self._counts = tf.Variable(tf.zeros([n], dtype=tf.int32))\r\n",
        "    self._values = tf.Variable(tf.ones([n], dtype=tf.float32))\r\n",
        "\r\n",
        "    policy = GreedyPolicy(n, self._values)\r\n",
        "    time_step_spec = policy.time_step_spec\r\n",
        "    action_spec = policy.action_spec\r\n",
        "\r\n",
        "    super(GreedyAgent, self).__init__(\r\n",
        "        time_step_spec=time_step_spec,\r\n",
        "        action_spec=action_spec,\r\n",
        "        policy=policy,\r\n",
        "        collect_policy=policy,\r\n",
        "        train_sequence_length=None)\r\n",
        "\r\n",
        "  def _initialize(self) -> Optional[tf.Operation]:\r\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\r\n",
        "\r\n",
        "  def _train(\r\n",
        "      self, \r\n",
        "      experience: types.NestedTensor, \r\n",
        "      weights: Optional[types.Tensor] = None) -> tf_agent.LossInfo:\r\n",
        "    # Get all necessary info from the trajectory\r\n",
        "    observation = experience.observation\r\n",
        "    action = experience.action\r\n",
        "    reward = experience.reward\r\n",
        "    \r\n",
        "    i = action.numpy()[0, 0]\r\n",
        "    c_update = np.zeros(self._n)\r\n",
        "    c_update[i] += 1\r\n",
        "\r\n",
        "    #self._counts = self._counts + c_update\r\n",
        "    tf.compat.v1.assign_add(self._counts, c_update)\r\n",
        "\r\n",
        "    values = self._values.numpy()\r\n",
        "    n = self._counts.numpy()[i]\r\n",
        "\r\n",
        "    values[i] = ((n - 1) / n) * values[i] + (1 / n) * reward.numpy()\r\n",
        "\r\n",
        "    tf.compat.v1.assign(self._values, values)\r\n",
        "\r\n",
        "    return tf_agent.LossInfo((), ())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ2zIGRd6hXi"
      },
      "source": [
        "environment = tf_py_environment.TFPyEnvironment(BannerEnvironment([0.25, 0.4, 0.67]))\r\n",
        "step = environment.reset()\r\n",
        "\r\n",
        "agent = GreedyAgent(3)\r\n",
        "\r\n",
        "for _ in range(100):\r\n",
        "  action_step = agent.collect_policy.action(step)  \r\n",
        "  next_step = environment.step(action_step.action)  \r\n",
        "  # Create trajectory nested \r\n",
        "  experience = trajectory_for_bandit(step, action_step, next_step)\r\n",
        "  # Train policy in the agent\r\n",
        "  agent.train(experience)\r\n",
        "  step = next_step\r\n",
        "\r\n",
        "print(f'Agent\\'s reward estimations={agent._values.numpy()} and counts={agent._counts.numpy()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b47tWwUVdgbV"
      },
      "source": [
        "#### Downloading the [MovieLens](https://grouplens.org/datasets/movielens/) (100K) dataset.\n",
        "\n",
        "**Dataset info**\n",
        "\n",
        "MovieLens data sets were collected by the GroupLens Research Project\n",
        "at the University of Minnesota.\n",
        "\n",
        "This data set consists of:\n",
        "* 100,000 ratings (1-5) from 943 users on 1682 movies.\n",
        "* Each user has rated at least 20 movies.\n",
        "* Simple demographic info for the users (age, gender, occupation, zip)\n",
        "\n",
        "The data was collected through the MovieLens web site\n",
        "(movielens.umn.edu) during the seven-month period from September 19th,\n",
        "1997 through April 22nd, 1998. This data has been cleaned up - users\n",
        "who had less than 20 ratings or did not have complete demographic\n",
        "information were removed from this data set. Detailed descriptions of\n",
        "the data file can be found at the end of this file.\n",
        "\n",
        "Neither the University of Minnesota nor any of the researchers\n",
        "involved can guarantee the correctness of the data, its suitability\n",
        "for any particular purpose, or the validity of results based on the\n",
        "use of the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8_xdGprnyqb"
      },
      "source": [
        "print(\"Downloading movielens data...\")\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    http://files.grouplens.org/datasets/movielens/ml-100k.zip \\\n",
        "    -O ./movielens.zip\n",
        "\n",
        "zip_ref = zipfile.ZipFile('movielens.zip', \"r\")\n",
        "zip_ref.extractall()\n",
        "\n",
        "print(\"Done. Dataset contains:\")\n",
        "print(zip_ref.read('ml-100k/u.info').decode())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBsEus-bdkRM"
      },
      "source": [
        "#### Parameters -- Feel Free to Play Around"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2Tf9d1NcbF9"
      },
      "source": [
        "RANK_K = 20 # @param {type:\"integer\"}\n",
        "NUM_ACTIONS = 20 # @param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvFAsPqLlmKQ"
      },
      "source": [
        "## Environment\n",
        "\n",
        "Implementation of the environment uses **MovieLens 100K dataset**. As described above, the dataset contains 100000 ratings from 943 users and 1682 movies. The environment can consider only the first $n$ of the dataset's movies. It can be set-up by `num_actions`. The number of \"known\" movies for the environment is equal to actions/arms.\n",
        "\n",
        "> Users without a rating (after selecting first $n$ movies) are removed from the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW6QuzcKX8gq"
      },
      "source": [
        "class MovieLensPyEnvironment(bandit_py_environment.BanditPyEnvironment):\n",
        "  \"\"\"Implements the MovieLens Bandit environment.\n",
        "  This environment implements the MovieLens 100K dataset, available at:\n",
        "  https://www.kaggle.com/prajitdatta/movielens-100k-dataset\n",
        "  This dataset contains 100K ratings from 943 users on 1682 items.\n",
        "  This csv list of:\n",
        "  user id | item id | rating | timestamp.\n",
        "  This environment computes a low-rank matrix factorization (using SVD) of the\n",
        "  data matrix A, such that: A ~= U * V.\n",
        "  The reward of recommending item `j` to user `i` is provided as A_{ij}.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               data_dir: Text,\n",
        "               rank_k: int,\n",
        "               batch_size: int = 1,\n",
        "               num_movies: int = 20,\n",
        "               name: Optional[Text] = 'movielens'):\n",
        "    \"\"\"Initializes the MovieLens Bandit environment.\n",
        "    Args:\n",
        "      data_dir: (string) Directory where the data lies (in text form).\n",
        "      rank_k : (int) Which rank to use in the matrix factorization.\n",
        "      batch_size: (int) Number of observations generated per call.\n",
        "      num_movies: (int) Only the first `num_movies` movies will be used by the\n",
        "        environment. The rest is cut out from the data.\n",
        "      name: The name of this environment instance.\n",
        "    \"\"\"\n",
        "    self._num_actions = num_movies\n",
        "    self._batch_size = batch_size\n",
        "    self._context_dim = rank_k\n",
        "\n",
        "    # Compute the matrix factorization.\n",
        "    #self._data_matrix = dataset_utilities.load_movielens_data(data_dir)\n",
        "    self._data_matrix = load_movielens_data(data_dir)\n",
        "    # Keep only the first items.\n",
        "    self._data_matrix = self._data_matrix[:, :num_movies]\n",
        "    # Filter the users with no iterm rated.\n",
        "    nonzero_users = list(np.nonzero(np.sum(self._data_matrix, axis=1) > 0.0)[0])\n",
        "    self._data_matrix = self._data_matrix[nonzero_users, :]\n",
        "    self._effective_num_users = len(nonzero_users)\n",
        "\n",
        "    # Compute the SVD.\n",
        "    u, s, vh = np.linalg.svd(self._data_matrix, full_matrices=False)\n",
        "\n",
        "    # Keep only the largest singular values.\n",
        "    self._u_hat = u[:, :rank_k] * np.sqrt(s[:rank_k])\n",
        "    self._v_hat = np.transpose(\n",
        "        np.transpose(vh[:rank_k, :]) * np.sqrt(s[:rank_k]))\n",
        "    self._approx_ratings_matrix = np.matmul(self._u_hat, self._v_hat)\n",
        "\n",
        "    self._current_users = np.zeros(batch_size, dtype=np.int32)\n",
        "    self._previous_users = np.zeros(batch_size, dtype=np.int32)\n",
        "\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(),\n",
        "        dtype=np.int32,\n",
        "        minimum=0,\n",
        "        maximum=self._num_actions - 1,\n",
        "        name='action')\n",
        "    observation_spec = array_spec.ArraySpec(\n",
        "        shape=(self._context_dim,), dtype=np.float64, name='observation')\n",
        "    self._time_step_spec = ts.time_step_spec(observation_spec)\n",
        "    self._observation = np.zeros((self._batch_size, self._context_dim))\n",
        "\n",
        "    self._optimal_action_table = np.argmax(\n",
        "        self._approx_ratings_matrix, axis=1)\n",
        "    self._optimal_reward_table = np.max(\n",
        "        self._approx_ratings_matrix, axis=1)\n",
        "\n",
        "    super(MovieLensPyEnvironment, self).__init__(\n",
        "        observation_spec, self._action_spec)\n",
        "\n",
        "  @property\n",
        "  def batch_size(self):\n",
        "    return self._batch_size\n",
        "\n",
        "  @property\n",
        "  def batched(self):\n",
        "    return True\n",
        "\n",
        "  def _observe(self):\n",
        "    \"\"\"Returns the u vectors of a random sample of users.\"\"\"\n",
        "    sampled_users = random.sample(\n",
        "        range(self._effective_num_users), self._batch_size)\n",
        "    self._previous_users = self._current_users\n",
        "    self._current_users = sampled_users\n",
        "    batched_observations = self._u_hat[sampled_users]\n",
        "    return batched_observations\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    \"\"\"Computes the reward for the input actions.\"\"\"\n",
        "    rewards = []\n",
        "    for i, j in zip(self._current_users, action):\n",
        "      rewards.append(self._approx_ratings_matrix[i, j])\n",
        "    return np.array(rewards)\n",
        "\n",
        "  def compute_optimal_action(self):\n",
        "    return self._optimal_action_table[self._previous_users]\n",
        "\n",
        "  def compute_optimal_reward(self):\n",
        "    return self._optimal_reward_table[self._previous_users]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxo7RipFdrPk"
      },
      "source": [
        "Now we are equipped to initialize our environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWLNxLEiT05z"
      },
      "source": [
        "env = MovieLensPyEnvironment('./ml-100k/u.data', RANK_K, 1, num_movies=NUM_ACTIONS)\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4IcbZr5duJt"
      },
      "source": [
        "Below we can check what this environment produces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcRZbsaSUCEh"
      },
      "source": [
        "print('Observation spec:', tf_env.observation_spec())\n",
        "print('An observation: ', tf_env.reset().observation.numpy())\n",
        "\n",
        "action = tf.zeros(1, dtype=tf.int32)\n",
        "time_step = tf_env.step(action)\n",
        "\n",
        "print(f'For users={env._previous_users}, we selected action={action.numpy()} (optimal={tf_env.compute_optimal_action()})')\n",
        "print(f'For users={env._previous_users}, we received reward={time_step.reward.numpy()} (optimal={tf_env.compute_optimal_reward()})')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTjTE8ThV85o"
      },
      "source": [
        "## Policy - LinerUCB\n",
        "\n",
        "As we leant in UCB example, the Upper Confidence Bounds (UCB) algorithm measures potential by an upper confidence bound of the reward value, $\\hat{U}_{t}(a)$, so that the true value is below with bound $Q(a) \\leq \\hat{Q}_t(a) + \\hat{U}_t(a)$ with high probability. The upper bound $\\hat{U}_t(a)$ is a function of $N_t(a)$; a larger number of trials $N_t(a)$ should give us a smaller bound $\\hat{U}_t(a)$, see picture below.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://miro.medium.com/max/4800/1*p_4mvZ6r6ddbShd7tOT0sw.png\" alt=\"source: https://towardsdatascience.com/recommender-systems-using-linucb-a-contextual-multi-armed-bandit-approach-35a6f0eb6c4\" width=\"600\"/>\n",
        "</center>\n",
        "\n",
        "<!--![](https://miro.medium.com/max/4800/1*p_4mvZ6r6ddbShd7tOT0sw.png \"source: https://towardsdatascience.com/recommender-systems-using-linucb-a-contextual-multi-armed-bandit-approach-35a6f0eb6c4\")-->\n",
        "\n",
        "For contextual bandits UCB, the expected payoff of an action is assumed to be linear in its d-dimensional feature vector $X$ with some unknown coefficient vector $\\theta$.\n",
        "\n",
        "$$\n",
        "E\\left[r_{t,a}|x_{t,a}\\right] = x^{T}_{t,a}\\theta^{\\ast}_{a}\n",
        "$$\n",
        "\n",
        "An upper confidence bound has to be calculated for each action for the algorithm to be able to choose an arm at every trial. The strategy for choosing the action at every trial $t$ is formalised as\n",
        "\n",
        "$$\n",
        "a_{t} \\stackrel{def}{=} argmax\\left(x^{T}_{t,a}\\hat{\\theta}_{a} + \\alpha \\sqrt{x^{T}_{t,a} A^{-1} x_{t,a}} \\right),\n",
        "$$\n",
        "\n",
        "where $\\hat{\\theta} = A^{-1}b$.\n",
        "\n",
        "<br/>\n",
        "\n",
        "**TASK**: Fill in the missing pieces of code and create:\n",
        "\n",
        "1. computation of the confidence intervals,\n",
        "1. choosing the next action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10eDwxGDV-n6"
      },
      "source": [
        "class LinearUCBPolicy(linear_bandit_policy.LinearBanditPolicy):\n",
        "  \"\"\"LinearUCB policy is simplified version of LinearBanditPolicy from tf_agente.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: types.BoundedTensorSpec,\n",
        "               variable_collection: tf.Module,\n",
        "               time_step_spec: Optional[types.TimeStep] = None,\n",
        "               alpha: float = 1.0,\n",
        "               tikhonov_weight: float = 1.0,\n",
        "               name: Optional[Text] = None):\n",
        "    super(LinearUCBPolicy, self).__init__(\n",
        "        action_spec,\n",
        "        cov_matrix=variable_collection.cov_matrix_list,\n",
        "        data_vector=variable_collection.data_vector_list,\n",
        "        num_samples=variable_collection.num_samples_list,\n",
        "        time_step_spec=time_step_spec,\n",
        "        alpha=alpha,\n",
        "        tikhonov_weight=tikhonov_weight,\n",
        "        name=name)\n",
        "\n",
        "  def _distribution(self, time_step, policy_state):\n",
        "    observation = tf.nest.map_structure(lambda o: tf.cast(o, dtype=self._dtype),\n",
        "                                        time_step.observation)\n",
        "    \n",
        "    current_observation = tf.reshape(\n",
        "        observation, [-1, self._global_context_dim])\n",
        "\n",
        "    est_rewards = []\n",
        "    confidence_intervals = []\n",
        "\n",
        "    for model_index in range(self._num_actions):\n",
        "      a = self._cov_matrix[model_index]\n",
        "      b = self._data_vector[model_index]\n",
        "      # Compute confidence interval for action(i): x^T*A^-1*x\n",
        "      # 1: A^-1*x -> A^-1x - A = a + tikhonow * I\n",
        "      # HINT 1: Be aware of `current_observation`, may be it will need transpose\n",
        "      # HINT 2: Identity matrix must have shape of the context (self._overall_context_dim)\n",
        "      a_inv_x = linalg.conjugate_gradient_solve(\n",
        "          # YOUR CODE HERE,\n",
        "          # YOUR CODE HERE\n",
        "          ) \n",
        "      # 2: x^T*A^-1x -> confidence interval of action(i)\n",
        "      # HINT 3: Use tf.matmul\n",
        "      ci = tf.reshape(\n",
        "          tf.linalg.tensor_diag_part(# YOUR CODE HERE), \n",
        "          [-1, 1])\n",
        "      \n",
        "      confidence_intervals.append(ci)\n",
        "      est_mean_reward = tf.einsum('j,jk->k', b,\n",
        "                                  a_inv_x)\n",
        "      est_rewards.append(est_mean_reward)\n",
        "    # Estimate rewards for every action\n",
        "    # HINT 4: mean reward must be reshaped - tf.reshape with [-1, 1]\n",
        "    optimistic_estimates = [\n",
        "        # YOUR CODE HERE\n",
        "        for mean_reward, confidence in zip(est_rewards, confidence_intervals)\n",
        "    ]\n",
        "    # Keeping the batch dimension during the squeeze, even if batch_size == 1.\n",
        "    rewards_for_argmax = tf.squeeze(\n",
        "        tf.stack(optimistic_estimates, axis=-1), axis=[1])\n",
        "    \n",
        "    # Choose the best action for every observation in the batch\n",
        "    chosen_actions = tf.argmax(\n",
        "        # YOUR CODE HERE, \n",
        "        axis=-1,\n",
        "        output_type=tf.nest.flatten(self._action_spec)[0].dtype)\n",
        "\n",
        "    action_distributions = tfp.distributions.Deterministic(loc=chosen_actions)\n",
        "\n",
        "    policy_info = policy_utilities.populate_policy_info(\n",
        "        None, chosen_actions, rewards_for_argmax,\n",
        "        tf.stack(est_rewards, axis=-1), self._emit_policy_info,\n",
        "        False)\n",
        "\n",
        "    return policy_step.PolicyStep(\n",
        "        action_distributions, policy_state, policy_info)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo3YPdpkxYsr"
      },
      "source": [
        "## Agent\n",
        "\n",
        "For contextual bandits UCB, the expected payoff of an action is assumed to be linear in its d-dimensional feature vector $X$ with some unknown coefficient vector $\\theta$.\n",
        "\n",
        "$$\n",
        "E\\left[r_{t,a}|x_{t,a}\\right] = x^{T}_{t,a}\\theta^{\\ast}_{a}\n",
        "$$\n",
        "\n",
        "This model is called disjoint since the parameters are not shared among different actions/arms. To solve for the coefficient vector $\\theta$ in the above equation ridge regression ([Tikhonov regularization](https://en.wikipedia.org/wiki/Tikhonov_regularization)) is applied to the training data. The whole algorithm is described below\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://raw.githubusercontent.com/pstanisl/mlprague-2021/main/img/linucb_algorithm.png\" alt=\"LinUCB algorithm\" width=\"400\"/>\n",
        "</center>\n",
        "\n",
        "<!--![linucb_algorithm.png](https://raw.githubusercontent.com/pstanisl/mlprague-2021/main/img/linucb_algorithm.png =100x)-->\n",
        "\n",
        "> More details about the algorithm can be found in the [A contextual-bandit approach to\n",
        "personalized news article recommendation](https://arxiv.org/pdf/1003.0146.pdf) and [Linear Upper Confidence Bound Algorithm for Contextual Bandit Problem with Piled Rewards](https://khhuang.me/docs/pakdd2016linucbpr.pdf) papers.\n",
        "\n",
        "The LinearAgent with `LinearUCBPolicy` agent implements the identically named Bandit algorithm, which estimates the parameter of the linear reward function while also maintains a confidence ellipsoid around the estimate. The agent chooses the action/arm that has the highest estimated expected reward, assuming that the parameter lies within the confidence ellipsoid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2zoC7YYVzxj"
      },
      "source": [
        "def sum_reward_weighted_observations(r: types.Tensor,\n",
        "                                     x: types.Tensor) -> types.Tensor:\n",
        "  \"\"\"Calculates an update used by some Bandit algorithms.\n",
        "  Given an observation `x` and corresponding reward `r`, the weigthed\n",
        "  observations vector (denoted `b` here) should be updated as `b = b + r * x`.\n",
        "  This function calculates the sum of weighted rewards for batched\n",
        "  observations `x`.\n",
        "\n",
        "  Args:\n",
        "    r: a `Tensor` of shape [`batch_size`]. This is the rewards of the batched\n",
        "      observations.\n",
        "    x: a `Tensor` of shape [`batch_size`, `context_dim`]. This is the matrix\n",
        "      with the (batched) observations.\n",
        "      \n",
        "  Returns:\n",
        "    The update that needs to be added to `b`. Has the same shape as `b`.\n",
        "    If the observation matrix `x` is empty, a zero vector is returned.\n",
        "  \"\"\"\n",
        "  batch_size = tf.shape(x)[0]\n",
        "\n",
        "  return tf.reduce_sum(tf.reshape(r, [batch_size, 1]) * x, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMo6hdUpD6hO"
      },
      "source": [
        "class LinearAgent(lin_agent.LinearBanditAgent):\n",
        "  \"\"\"Simplified implentation of an agent that maintains linear reward \n",
        "  estimates and their uncertainties.\n",
        "  \n",
        "  Original implementation can be found here: http://bit.ly/3kk7v3D\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "              time_step_spec: types.TimeStep,\n",
        "              action_spec: types.BoundedTensorSpec,\n",
        "              policy_class: linear_bandit_policy.LinearBanditPolicy = LinearUCBPolicy,\n",
        "              alpha: float = 1.0,\n",
        "              tikhonov_weight: float = 1.0,\n",
        "              dtype: tf.DType = tf.float32,\n",
        "              name: Optional[Text] = None):\n",
        "\n",
        "    super(LinearAgent, self).__init__(\n",
        "        lin_agent.ExplorationPolicy.linear_ucb_policy,\n",
        "        time_step_spec=time_step_spec,\n",
        "        action_spec=action_spec,\n",
        "        alpha=alpha,\n",
        "        tikhonov_weight=tikhonov_weight,\n",
        "        dtype=dtype,\n",
        "        name=name\n",
        "    )\n",
        "\n",
        "    self._as_trajectory = data_converter.AsTrajectory(\n",
        "      self.data_context, sequence_length=None)\n",
        "\n",
        "    self._policy = self._policy = policy_class(\n",
        "        action_spec=action_spec,\n",
        "        variable_collection=self._variable_collection,\n",
        "        time_step_spec=time_step_spec,\n",
        "        alpha=alpha,\n",
        "        tikhonov_weight=tikhonov_weight\n",
        "    )\n",
        "  \n",
        "  def _train(self, experience, weights=None):\n",
        "    \"\"\"Updates the policy based on the data in `experience`.\n",
        "    Note that `experience` should only contain data points that this agent has\n",
        "    not previously seen. If `experience` comes from a replay buffer, this buffer\n",
        "    should be cleared between each call to `train`.\n",
        "    Args:\n",
        "      experience: A batch of experience data in the form of a `Trajectory`.\n",
        "      weights: Unused.\n",
        "    Returns:\n",
        "        A `LossInfo` containing the loss *before* the training step is taken.\n",
        "        In most cases, if `weights` is provided, the entries of this tuple will\n",
        "        have been calculated with the weights.  Note that each Agent chooses\n",
        "        its own method of applying weights.\n",
        "    \"\"\"\n",
        "    experience = self._as_trajectory(experience)\n",
        "\n",
        "    del weights  # unused\n",
        "\n",
        "    reward, action, observation, batch_size = self._process_experience(\n",
        "        experience)\n",
        "    \n",
        "    for k in range(self._num_models):\n",
        "      # Create identity matrix used as a mask\n",
        "      diag_mask = tf.linalg.tensor_diag(\n",
        "          tf.cast(tf.equal(action, k), self._dtype))\n",
        "      # Get an observation for the action from the observation\n",
        "      observations_for_arm = tf.matmul(diag_mask, observation)\n",
        "      rewards_for_arm = tf.matmul(diag_mask, tf.reshape(reward, [-1, 1]))\n",
        "      num_samples_for_arm_current = tf.reduce_sum(diag_mask)\n",
        "      \n",
        "      tf.compat.v1.assign_add(self._num_samples_list[k],\n",
        "                              num_samples_for_arm_current)\n",
        "      num_samples_for_arm_total = self._num_samples_list[k].read_value()\n",
        "\n",
        "      # Update the covariance matrix `a` and the weighted sum of rewards `b`\n",
        "      # using a forgetting factor `gamma`.\n",
        "      \n",
        "      # YOUR CODE GOES HERE\n",
        "      # HINT 1: for computing x*r you can use `sum_reward_weighted_observations`\n",
        "      # HINT 2: self._cov_matrix_list is a List of tf.Variables (also self._data_vector_list)\n",
        "      \n",
        "\n",
        "\n",
        "      # END OF YOUR CODE\n",
        "\n",
        "      # Update real variables\n",
        "      tf.compat.v1.assign(self._cov_matrix_list[k], # YOUR CODE HERE) \n",
        "      tf.compat.v1.assign(self._data_vector_list[k], # YOUR CODE HERE) \n",
        "\n",
        "    loss = -1. * # YOUR CODE HERE\n",
        "    self.compute_summaries(loss)\n",
        "\n",
        "    self._train_step_counter.assign_add(batch_size)\n",
        "\n",
        "    return tf_agent.LossInfo(loss=(loss), extra=())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huc03djLkNyt"
      },
      "source": [
        "Helper function for creating an instance of the `LinearAgent` with a linear policy like our `LinearUCBPolicy`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkJm2_WpgfaL"
      },
      "source": [
        "def get_agent(\n",
        "    environment, \n",
        "    policy_class: linear_bandit_policy.LinearBanditPolicy = LinearUCBPolicy,\n",
        "    tikhonov_weight: float = 0.001, \n",
        "    alpha: float = 10.0\n",
        "):  \n",
        "  return LinearAgent(\n",
        "    time_step_spec=environment.time_step_spec(),\n",
        "    action_spec=environment.action_spec(),\n",
        "    policy_class=policy_class,\n",
        "    tikhonov_weight=tikhonov_weight,\n",
        "    alpha=alpha,\n",
        "    dtype=tf.float32\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG8LVv2rW1sK"
      },
      "source": [
        "agent = get_agent(\n",
        "    tf_env, policy_class=LinearUCBPolicy, tikhonov_weight=0.001, alpha=10.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nE2kDQ2kcSo"
      },
      "source": [
        "Let have a look at the data specification in the agent. The `training_data_spec` attribute of the agent specifies what elements and structure the training data should have. The `training_data_spec.observation` specificate the structure of the context vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPg7h-hIlTnx"
      },
      "source": [
        "print('training data spec: ', agent.training_data_spec)\n",
        "print('observation spec in training: ', agent.training_data_spec.observation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDjYCtBUl2j4"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now we put together all the components that we introduced above: the environment, the policy, and the agent. We run the policy on the environment and output training data with the help of a driver, and train the agent on the data.\n",
        "\n",
        "#### Metrics\n",
        "\n",
        "Important of the training are metrics. If you read some materials you can find, that bandits' most important metric is regret, calculated as the difference between the reward collected by the agent and the expected reward of an oracle policy that has access to the reward functions of the environment. The [RegretMetric](https://github.com/tensorflow/agents/blob/master/tf_agents/bandits/metrics/tf_metrics.py) thus needs a `baseline_reward_fn` function that calculates the best achievable expected reward given an observation. In our example, the optimal reward is computed in the `MovieLensPyEnvironment.compute_optimal_reward` from the approximation of the rating.\n",
        "\n",
        "> In reality, we usually do not have access to an oracle policy, so the regret is hard to get. Thus, the cumulative reward or other metric is often used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTmTQNtQpk_a"
      },
      "source": [
        "def get_metrics(environment):\n",
        "  optimal_reward_fn = functools.partial(\n",
        "        environment_utilities.compute_optimal_reward_with_movielens_environment,\n",
        "        environment=tf_env)\n",
        "  optimal_action_fn = functools.partial(\n",
        "        environment_utilities.compute_optimal_action_with_movielens_environment,\n",
        "        environment=tf_env)\n",
        "  \n",
        "  regret_metric = tf_bandit_metrics.RegretMetric(\n",
        "      optimal_reward_fn, \n",
        "      name='regret'\n",
        "  )\n",
        "  suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n",
        "      optimal_action_fn,\n",
        "      name='suboptimal_arms'\n",
        "  )\n",
        "  \n",
        "  return [regret_metric, suboptimal_arms_metric]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7zx1AUBe5_f"
      },
      "source": [
        "We will put it all together in `run` function to run the training loop of our implementation of bandits' movie recommendations. The driver below is a helper object and takes care of choosing actions using the policy, storing rewards of chosen actions in the replay buffer, calculating the predefined regret metric, and executing the agent's training step. You can find more info about the driver [here](https://www.tensorflow.org/agents/tutorials/4_drivers_tutorial)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5foepaVcv9Y"
      },
      "source": [
        "def run(\n",
        "    environment, \n",
        "    agent, \n",
        "    iterations, \n",
        "    steps_per_loop,\n",
        "    additional_metrics=()\n",
        "):\n",
        "  replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "      data_spec=agent.policy.trajectory_spec,\n",
        "      batch_size=environment.batch_size,\n",
        "      max_length=steps_per_loop)\n",
        "  \n",
        "  metrics = [] + list(additional_metrics)\n",
        "  ret_metrics = dict([(m.name, []) for m in metrics])\n",
        "\n",
        "  observers = [replay_buffer.add_batch] + metrics\n",
        "\n",
        "  driver = dynamic_step_driver.DynamicStepDriver(\n",
        "      env=environment,\n",
        "      policy=agent.collect_policy,\n",
        "      num_steps=steps_per_loop * environment.batch_size,\n",
        "      observers=observers)\n",
        "\n",
        "  regret_values = []\n",
        "\n",
        "  for _ in trange(num_iterations):\n",
        "    driver.run()\n",
        "    loss_info = agent.train(replay_buffer.gather_all())\n",
        "    replay_buffer.clear()\n",
        "    # Log metrics value\n",
        "    for metric in metrics:\n",
        "      ret_metrics[metric.name].append(metric.result())\n",
        "\n",
        "  return ret_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7gENChle_nn"
      },
      "source": [
        "Down below is the code for creating all necessary instances. Note that two parameters together specify the number of steps taken. `num_iterations` specifies how many times we run the trainer loop, while the driver will take `steps_per_loop` steps per iteration. The main reason behind keeping both of these parameters is that some operations are done per iteration, while the driver does some in every step. For example, the agent's train function is only called once per iteration. The trade-off here is that if we train more often, our policy is \"fresher\"; on the other hand, training in bigger batches might be more time-efficient. `batch_size` defines how many actions are generated through one step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yxB1q29LqWV"
      },
      "source": [
        "batch_size =   32# @param {type:\"integer\"}\n",
        "num_iterations =   150# @param {type:\"integer\"}\n",
        "steps_per_loop =   2# @param {type:\"integer\"}\n",
        "agent_alpha = 2.0  # @param {type: \"number\"}\n",
        "tikhonov_weight = 0.001  # @param {type: \"number\"}\n",
        "\n",
        "env = MovieLensPyEnvironment(\n",
        "    './ml-100k/u.data', \n",
        "    rank_k=RANK_K,\n",
        "    batch_size=batch_size, \n",
        "    num_movies=NUM_ACTIONS\n",
        ")\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "tf_env.reset()\n",
        "\n",
        "agent = get_agent(\n",
        "    tf_env,\n",
        "    policy_class=LinearUCBPolicy,\n",
        "    tikhonov_weight=tikhonov_weight,\n",
        "    alpha=agent_alpha\n",
        ")\n",
        "\n",
        "additional_metrics = get_metrics(tf_env)\n",
        "\n",
        "metrics = run(\n",
        "    tf_env, \n",
        "    agent, \n",
        "    iterations=num_iterations,\n",
        "    steps_per_loop=steps_per_loop,\n",
        "    additional_metrics=additional_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtoJvh6xeBxu"
      },
      "source": [
        "Now let's see the result. After running the last code snippet, the resulting plot (hopefully) shows that the average regret is going down as the agent is trained and the policy gets better in figuring out what the right action is, given the observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqo41BBZXLZh"
      },
      "source": [
        "plot_regret(metrics['regret'], {'algorithm': 'LinUCB'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rc93uPhd5I1"
      },
      "source": [
        "## Linear Thomson Sampling policy\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://raw.githubusercontent.com/pstanisl/mlprague-2021/main/img/lints_algorithm.png\" alt=\"LinTS algorithm\" width=\"400\"/>\n",
        "</center>\n",
        "\n",
        "> A detailed explanation for the two above cases can be found in the paper\n",
        "[\"Thompson Sampling for Contextual Bandits with Linear Payoffs\"](http://proceedings.mlr.press/v28/agrawal13.pdf),\n",
        "Shipra Agrawal, Navin Goyal, ICML 2013\n",
        ", and its [supplementary material](http://proceedings.mlr.press/v28/agrawal13-supp.pdf).\n",
        "\n",
        "<br />\n",
        "\n",
        "**TASK**: Fill in missing code for\n",
        "\n",
        "1. computing confidence interval,\n",
        "1. sample actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKxJtkHrwTaS"
      },
      "source": [
        "class LinearTSPolicy(linear_bandit_policy.LinearBanditPolicy):\n",
        "  \"\"\"LinearTS policy is simplified version of LinearBanditPolicy from tf_agents.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: types.BoundedTensorSpec,\n",
        "               variable_collection: tf.Module,\n",
        "               time_step_spec: Optional[types.TimeStep] = None,\n",
        "               alpha: float = 1.0,\n",
        "               tikhonov_weight: float = 1.0,\n",
        "               name: Optional[Text] = None):\n",
        "    super(LinearTSPolicy, self).__init__(\n",
        "        action_spec,\n",
        "        cov_matrix=variable_collection.cov_matrix_list,\n",
        "        data_vector=variable_collection.data_vector_list,\n",
        "        num_samples=variable_collection.num_samples_list,\n",
        "        time_step_spec=time_step_spec,\n",
        "        alpha=alpha,\n",
        "        tikhonov_weight=tikhonov_weight,\n",
        "        name=name)\n",
        "\n",
        "  def _distribution(self, time_step, policy_state):\n",
        "    observation = tf.nest.map_structure(lambda o: tf.cast(o, dtype=self._dtype),\n",
        "                                        time_step.observation)\n",
        "    \n",
        "    current_observation = tf.reshape(\n",
        "        observation, [-1, self._global_context_dim])\n",
        "\n",
        "    est_rewards = []\n",
        "    confidence_intervals = []\n",
        "\n",
        "    for model_index in range(self._num_actions):\n",
        "      # Compute confidence interval for action(i): x^T*A^-1*x\n",
        "      \n",
        "      # YOUR CODE GOES HERE  \n",
        "\n",
        "\n",
        "      # END OF YOUR CODE\n",
        "      \n",
        "      confidence_intervals.append(ci)\n",
        "      est_mean_reward = tf.einsum('j,jk->k', self._data_vector[model_index],\n",
        "                                  a_inv_x)\n",
        "      est_rewards.append(est_mean_reward)\n",
        "    # Sample from the Normapl distribution\n",
        "    # HINT: Use tf.stack, tf.squeeze, tf.sqrt\n",
        "    mu_sampler = tfp.distributions.Normal(\n",
        "          loc=# YOUR CODE HERE, \n",
        "          scale=self._alpha * # YOUR CODE HERE \n",
        "    rewards_for_argmax = mu_sampler.sample()\n",
        "    # Choose the best action for every observation in the batch\n",
        "    chosen_actions = tf.argmax(\n",
        "         # YOUR CODE HERE,\n",
        "        axis=-1,\n",
        "        output_type=tf.nest.flatten(self._action_spec)[0].dtype)\n",
        "\n",
        "    action_distributions = tfp.distributions.Deterministic(loc=chosen_actions)\n",
        "\n",
        "    policy_info = policy_utilities.populate_policy_info(\n",
        "        None, chosen_actions, rewards_for_argmax,\n",
        "        tf.stack(est_rewards, axis=-1), self._emit_policy_info,\n",
        "        False)\n",
        "\n",
        "    return policy_step.PolicyStep(\n",
        "        action_distributions, policy_state, policy_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltBIXUg9mI7u"
      },
      "source": [
        "Let's repeat the training with `LinearTSPolicy` and see the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYdibk130cGM"
      },
      "source": [
        "batch_size =    32# @param {type:\"integer\"}\n",
        "num_iterations =   150# @param {type:\"integer\"}\n",
        "steps_per_loop =   2# @param {type:\"integer\"}\n",
        "agent_alpha = 2.0  # @param {type: \"number\"}\n",
        "tikhonov_weight = 0.001  # @param {type: \"number\"}\n",
        "\n",
        "tf_env.reset()\n",
        "\n",
        "agent = get_agent(\n",
        "  tf_env,\n",
        "  policy_class=LinearTSPolicy,\n",
        "  tikhonov_weight=tikhonov_weight,\n",
        "  alpha=agent_alpha\n",
        ")\n",
        "\n",
        "additional_metrics = get_metrics(tf_env)\n",
        "\n",
        "metrics = run(\n",
        "  tf_env, \n",
        "  agent, \n",
        "  iterations=num_iterations,\n",
        "  steps_per_loop=steps_per_loop,\n",
        "  additional_metrics=additional_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEAkVJWil4nG"
      },
      "source": [
        "plot_regret(metrics['regret'], {'algorithm': 'LinTS'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBsLy4I8l6bh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}